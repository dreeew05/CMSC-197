{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Github Link: https://github.com/dreeew05/CMSC-197/tree/main/Assignment%203"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Regex for removing unwanted characters\n",
    "import re\n",
    "\n",
    "# To read email\n",
    "from email import policy\n",
    "from email.parser import BytesParser\n",
    "\n",
    "# To count common words\n",
    "from collections import Counter\n",
    "\n",
    "# for evaluation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_PATH = \"trec06p-cs280/\"\n",
    "COMMON_WORD_COUNT = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the stop words and convert into array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'able', 'about', 'above', 'abst']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = open('stop_words.txt').read().splitlines()\n",
    "\n",
    "# For visualization purposes\n",
    "stop_words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'file_path': [],\n",
    "    'category': []\n",
    "}\n",
    "\n",
    "# Force category to store int\n",
    "# 0 for ham\n",
    "# 1 for spam\n",
    "# df = pd.DataFrame(data).astype({\"category\": \"int\"})\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Preprocessing}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/000/000</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/000/001</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/000/002</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/000/003</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/000/004</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37817</th>\n",
       "      <td>data/126/017</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37818</th>\n",
       "      <td>data/126/018</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37819</th>\n",
       "      <td>data/126/019</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37820</th>\n",
       "      <td>data/126/020</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37821</th>\n",
       "      <td>data/126/021</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37822 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          file_path category\n",
       "0      data/000/000      ham\n",
       "1      data/000/001     spam\n",
       "2      data/000/002     spam\n",
       "3      data/000/003      ham\n",
       "4      data/000/004     spam\n",
       "...             ...      ...\n",
       "37817  data/126/017     spam\n",
       "37818  data/126/018     spam\n",
       "37819  data/126/019     spam\n",
       "37820  data/126/020     spam\n",
       "37821  data/126/021     spam\n",
       "\n",
       "[37822 rows x 2 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_path = f\"{FOLDER_PATH}labels\"\n",
    "with open(labels_path) as f:\n",
    "    # Remove ../to mitigate file access errors\n",
    "    str_to_remove = \"../\"\n",
    "    for line in f:\n",
    "        category, path = line.split()\n",
    "        # category_code = 0 if category == \"ham\" else 1\n",
    "        clean_path = path.replace(str_to_remove, '')\n",
    "        new_row = pd.DataFrame([[clean_path, category]], columns=[\"file_path\", \"category\"])\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the email:\n",
    "\n",
    "- Remove alphanumeric characters\n",
    "- Remove punctuation marks\n",
    "- Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_email(email_body):\n",
    "    # [^a-zA-Z\\s]+ => For non-alphabetic and non-whitespace (punctuations, new line, tab)\n",
    "    # \\s+ = For one or more whitespace characters\n",
    "    pattern = r\"[^a-zA-Z\\s]+|\\s+\"  # Combine both patterns\n",
    "    clean_message = re.sub(pattern, \" \", email_body).strip().lower()\n",
    "\n",
    "    # Split and remove stop words in a single step\n",
    "    return [word for word in clean_message.split() if word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating to each mail:\n",
    "- Clean each mail\n",
    "- Tokenize the clean mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents_arr = []\n",
    "\n",
    "for path in df[\"file_path\"]:\n",
    "    current_file_path = f\"{FOLDER_PATH}{path}\"\n",
    "    with open(current_file_path, \"rb\") as f:\n",
    "        raw_email = f.read()\n",
    "\n",
    "    # Parse email content\n",
    "    msg = BytesParser(policy=policy.default).parsebytes(raw_email)\n",
    "\n",
    "    # Extract body (defaulting to empty string in case of issues)\n",
    "    body = \"\"\n",
    "\n",
    "    # Define a function to decode email parts safely\n",
    "    def decode_payload(part):\n",
    "        try:\n",
    "            charset = part.get_content_charset() or \"utf-8\"\n",
    "            return part.get_payload(decode=True).decode(charset)\n",
    "        except (LookupError, UnicodeDecodeError):\n",
    "            return part.get_payload(decode=True).decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "    # Check for multipart or single-part message\n",
    "    if msg.is_multipart():\n",
    "        for part in msg.iter_parts():\n",
    "            if part.get_content_type() == \"text/plain\":\n",
    "                body = decode_payload(part)\n",
    "                break\n",
    "    else:\n",
    "        body = decode_payload(msg)\n",
    "\n",
    "    # Clean the email body and append the word list\n",
    "    word_list = clean_email(body)\n",
    "    contents_arr.append(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding another column to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_list'] = contents_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into three groups:\n",
    "\n",
    "- Training set for ham\n",
    "- Training set for spam\n",
    "- Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>category</th>\n",
       "      <th>word_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/000/000</td>\n",
       "      <td>ham</td>\n",
       "      <td>[mailing, list, queried, weeks, ago, running, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/000/001</td>\n",
       "      <td>spam</td>\n",
       "      <td>[luxury, watches, buy, rolex, rolex, cartier, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/000/002</td>\n",
       "      <td>spam</td>\n",
       "      <td>[academic, qualifications, prestigious, acc, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/000/003</td>\n",
       "      <td>ham</td>\n",
       "      <td>[greetings, verify, subscription, plan, fans, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/000/004</td>\n",
       "      <td>spam</td>\n",
       "      <td>[chauncey, conferred, luscious, continued, ton...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21295</th>\n",
       "      <td>data/070/295</td>\n",
       "      <td>spam</td>\n",
       "      <td>[http, high, biz, ez, xin, walla]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21296</th>\n",
       "      <td>data/070/296</td>\n",
       "      <td>spam</td>\n",
       "      <td>[special, offer, adobe, video, collection, ado...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21297</th>\n",
       "      <td>data/070/297</td>\n",
       "      <td>spam</td>\n",
       "      <td>[doctype, html, public, dtd, html, transitiona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21298</th>\n",
       "      <td>data/070/298</td>\n",
       "      <td>ham</td>\n",
       "      <td>[mounted, infrared, demodulator, hb, realised,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21299</th>\n",
       "      <td>data/070/299</td>\n",
       "      <td>spam</td>\n",
       "      <td>[http, tmqmct, overpace, net, suffering, pain,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21300 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          file_path category  \\\n",
       "0      data/000/000      ham   \n",
       "1      data/000/001     spam   \n",
       "2      data/000/002     spam   \n",
       "3      data/000/003      ham   \n",
       "4      data/000/004     spam   \n",
       "...             ...      ...   \n",
       "21295  data/070/295     spam   \n",
       "21296  data/070/296     spam   \n",
       "21297  data/070/297     spam   \n",
       "21298  data/070/298      ham   \n",
       "21299  data/070/299     spam   \n",
       "\n",
       "                                               word_list  \n",
       "0      [mailing, list, queried, weeks, ago, running, ...  \n",
       "1      [luxury, watches, buy, rolex, rolex, cartier, ...  \n",
       "2      [academic, qualifications, prestigious, acc, r...  \n",
       "3      [greetings, verify, subscription, plan, fans, ...  \n",
       "4      [chauncey, conferred, luscious, continued, ton...  \n",
       "...                                                  ...  \n",
       "21295                  [http, high, biz, ez, xin, walla]  \n",
       "21296  [special, offer, adobe, video, collection, ado...  \n",
       "21297  [doctype, html, public, dtd, html, transitiona...  \n",
       "21298  [mounted, infrared, demodulator, hb, realised,...  \n",
       "21299  [http, tmqmct, overpace, net, suffering, pain,...  \n",
       "\n",
       "[21300 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ham Training Set\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>category</th>\n",
       "      <th>word_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/000/000</td>\n",
       "      <td>ham</td>\n",
       "      <td>[mailing, list, queried, weeks, ago, running, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/000/003</td>\n",
       "      <td>ham</td>\n",
       "      <td>[greetings, verify, subscription, plan, fans, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>data/000/005</td>\n",
       "      <td>ham</td>\n",
       "      <td>[quiet, quiet, well, straw, poll, plan, running]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>data/000/006</td>\n",
       "      <td>ham</td>\n",
       "      <td>[working, departed, totally, bell, labs, recom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>data/000/010</td>\n",
       "      <td>ham</td>\n",
       "      <td>[greetings, mass, acknowledgement, signed, pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21270</th>\n",
       "      <td>data/070/270</td>\n",
       "      <td>ham</td>\n",
       "      <td>[equation, generate, prime, numbers, equation,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21271</th>\n",
       "      <td>data/070/271</td>\n",
       "      <td>ham</td>\n",
       "      <td>[equation, generate, prime, numbers, equation,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21288</th>\n",
       "      <td>data/070/288</td>\n",
       "      <td>ham</td>\n",
       "      <td>[dear, dmdx, users, guidance, generating, dmdx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21293</th>\n",
       "      <td>data/070/293</td>\n",
       "      <td>ham</td>\n",
       "      <td>[built, handyboard, works, great, testmotor, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21298</th>\n",
       "      <td>data/070/298</td>\n",
       "      <td>ham</td>\n",
       "      <td>[mounted, infrared, demodulator, hb, realised,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7523 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          file_path category  \\\n",
       "0      data/000/000      ham   \n",
       "3      data/000/003      ham   \n",
       "5      data/000/005      ham   \n",
       "6      data/000/006      ham   \n",
       "10     data/000/010      ham   \n",
       "...             ...      ...   \n",
       "21270  data/070/270      ham   \n",
       "21271  data/070/271      ham   \n",
       "21288  data/070/288      ham   \n",
       "21293  data/070/293      ham   \n",
       "21298  data/070/298      ham   \n",
       "\n",
       "                                               word_list  \n",
       "0      [mailing, list, queried, weeks, ago, running, ...  \n",
       "3      [greetings, verify, subscription, plan, fans, ...  \n",
       "5       [quiet, quiet, well, straw, poll, plan, running]  \n",
       "6      [working, departed, totally, bell, labs, recom...  \n",
       "10     [greetings, mass, acknowledgement, signed, pla...  \n",
       "...                                                  ...  \n",
       "21270  [equation, generate, prime, numbers, equation,...  \n",
       "21271  [equation, generate, prime, numbers, equation,...  \n",
       "21288  [dear, dmdx, users, guidance, generating, dmdx...  \n",
       "21293  [built, handyboard, works, great, testmotor, p...  \n",
       "21298  [mounted, infrared, demodulator, hb, realised,...  \n",
       "\n",
       "[7523 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam Training Set\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>category</th>\n",
       "      <th>word_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/000/001</td>\n",
       "      <td>spam</td>\n",
       "      <td>[luxury, watches, buy, rolex, rolex, cartier, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/000/002</td>\n",
       "      <td>spam</td>\n",
       "      <td>[academic, qualifications, prestigious, acc, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/000/004</td>\n",
       "      <td>spam</td>\n",
       "      <td>[chauncey, conferred, luscious, continued, ton...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>data/000/007</td>\n",
       "      <td>spam</td>\n",
       "      <td>[nbc, today, body, diet, beaches, magazines, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>data/000/008</td>\n",
       "      <td>spam</td>\n",
       "      <td>[oil, sector, going, crazy, weekly, gift, kkpt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21294</th>\n",
       "      <td>data/070/294</td>\n",
       "      <td>spam</td>\n",
       "      <td>[txt, add]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21295</th>\n",
       "      <td>data/070/295</td>\n",
       "      <td>spam</td>\n",
       "      <td>[http, high, biz, ez, xin, walla]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21296</th>\n",
       "      <td>data/070/296</td>\n",
       "      <td>spam</td>\n",
       "      <td>[special, offer, adobe, video, collection, ado...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21297</th>\n",
       "      <td>data/070/297</td>\n",
       "      <td>spam</td>\n",
       "      <td>[doctype, html, public, dtd, html, transitiona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21299</th>\n",
       "      <td>data/070/299</td>\n",
       "      <td>spam</td>\n",
       "      <td>[http, tmqmct, overpace, net, suffering, pain,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13777 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          file_path category  \\\n",
       "1      data/000/001     spam   \n",
       "2      data/000/002     spam   \n",
       "4      data/000/004     spam   \n",
       "7      data/000/007     spam   \n",
       "8      data/000/008     spam   \n",
       "...             ...      ...   \n",
       "21294  data/070/294     spam   \n",
       "21295  data/070/295     spam   \n",
       "21296  data/070/296     spam   \n",
       "21297  data/070/297     spam   \n",
       "21299  data/070/299     spam   \n",
       "\n",
       "                                               word_list  \n",
       "1      [luxury, watches, buy, rolex, rolex, cartier, ...  \n",
       "2      [academic, qualifications, prestigious, acc, r...  \n",
       "4      [chauncey, conferred, luscious, continued, ton...  \n",
       "7      [nbc, today, body, diet, beaches, magazines, h...  \n",
       "8      [oil, sector, going, crazy, weekly, gift, kkpt...  \n",
       "...                                                  ...  \n",
       "21294                                         [txt, add]  \n",
       "21295                  [http, high, biz, ez, xin, walla]  \n",
       "21296  [special, offer, adobe, video, collection, ado...  \n",
       "21297  [doctype, html, public, dtd, html, transitiona...  \n",
       "21299  [http, tmqmct, overpace, net, suffering, pain,...  \n",
       "\n",
       "[13777 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Set\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>category</th>\n",
       "      <th>word_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21300</th>\n",
       "      <td>data/071/000</td>\n",
       "      <td>spam</td>\n",
       "      <td>[hesitantly, derive, perverse, satisfaction, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21301</th>\n",
       "      <td>data/071/001</td>\n",
       "      <td>ham</td>\n",
       "      <td>[things, perform, experiment, display, will, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21302</th>\n",
       "      <td>data/071/002</td>\n",
       "      <td>spam</td>\n",
       "      <td>[best, offer, month, viggra, ci, ialis, vaiium...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21303</th>\n",
       "      <td>data/071/003</td>\n",
       "      <td>spam</td>\n",
       "      <td>[de, ar, wne, cr, doesn, matter, ow, real, st,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21304</th>\n",
       "      <td>data/071/004</td>\n",
       "      <td>spam</td>\n",
       "      <td>[special, offer, adobe, video, collection, ado...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37817</th>\n",
       "      <td>data/126/017</td>\n",
       "      <td>spam</td>\n",
       "      <td>[great, news, expec, ted, infinex, ventures, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37818</th>\n",
       "      <td>data/126/018</td>\n",
       "      <td>spam</td>\n",
       "      <td>[oil, sector, going, crazy, weekly, gift, kkpt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37819</th>\n",
       "      <td>data/126/019</td>\n",
       "      <td>spam</td>\n",
       "      <td>[http, vdtobj, docscan, info, suffering, pain,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37820</th>\n",
       "      <td>data/126/020</td>\n",
       "      <td>spam</td>\n",
       "      <td>[prosperous, future, increased, money, earning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37821</th>\n",
       "      <td>data/126/021</td>\n",
       "      <td>spam</td>\n",
       "      <td>[moat, coverall, cytochemistry, planeload, salk]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16522 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          file_path category  \\\n",
       "21300  data/071/000     spam   \n",
       "21301  data/071/001      ham   \n",
       "21302  data/071/002     spam   \n",
       "21303  data/071/003     spam   \n",
       "21304  data/071/004     spam   \n",
       "...             ...      ...   \n",
       "37817  data/126/017     spam   \n",
       "37818  data/126/018     spam   \n",
       "37819  data/126/019     spam   \n",
       "37820  data/126/020     spam   \n",
       "37821  data/126/021     spam   \n",
       "\n",
       "                                               word_list  \n",
       "21300  [hesitantly, derive, perverse, satisfaction, c...  \n",
       "21301  [things, perform, experiment, display, will, r...  \n",
       "21302  [best, offer, month, viggra, ci, ialis, vaiium...  \n",
       "21303  [de, ar, wne, cr, doesn, matter, ow, real, st,...  \n",
       "21304  [special, offer, adobe, video, collection, ado...  \n",
       "...                                                  ...  \n",
       "37817  [great, news, expec, ted, infinex, ventures, i...  \n",
       "37818  [oil, sector, going, crazy, weekly, gift, kkpt...  \n",
       "37819  [http, vdtobj, docscan, info, suffering, pain,...  \n",
       "37820  [prosperous, future, increased, money, earning...  \n",
       "37821   [moat, coverall, cytochemistry, planeload, salk]  \n",
       "\n",
       "[16522 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df = df[df['file_path'] < 'data/071']\n",
    "train_ham_df = train_df[train_df[\"category\"] == \"ham\"]\n",
    "train_spam_df = train_df[train_df['category'] == \"spam\"]\n",
    "\n",
    "test_df = df[df['file_path'] >= 'data/071']\n",
    "\n",
    "# For Visualization Purposes\n",
    "print('Training Set')\n",
    "display(train_df)\n",
    "print('Ham Training Set')\n",
    "display(train_ham_df)\n",
    "print('Spam Training Set')\n",
    "display(train_spam_df)\n",
    "print('Testing Set')\n",
    "display(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the 10000 most common words from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('http', 27587),\n",
       " ('font', 27472),\n",
       " ('td', 27416),\n",
       " ('br', 24631),\n",
       " ('width', 13978),\n",
       " ('tr', 12527),\n",
       " ('will', 11484),\n",
       " ('size', 11289),\n",
       " ('color', 7526),\n",
       " ('html', 7319)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_words = [word for sublist in train_df['word_list'] for word in sublist]\n",
    "\n",
    "word_count = Counter(training_words)\n",
    "top_common_words_with_freq = word_count.most_common(COMMON_WORD_COUNT)\n",
    "common_words = [word for word, _ in top_common_words_with_freq]\n",
    "\n",
    "# For vizualization purposes\n",
    "top_common_words_with_freq[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Creating the feature matrices}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_matrix(emails):\n",
    "    emails_num = len(emails)\n",
    "    feature_matrix = np.zeros((emails_num, COMMON_WORD_COUNT), dtype=int)\n",
    "\n",
    "    for i, email in enumerate(emails):\n",
    "        for word in email:\n",
    "            if word in common_words:\n",
    "                index = common_words.index(word)\n",
    "                feature_matrix[i, index] += 1\n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ham Matrix: \n",
      " [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "Spam Matrix: \n",
      " [[1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [6 4 0 ... 0 0 0]\n",
      " [2 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "ham_fm = create_feature_matrix(train_ham_df['word_list'])\n",
    "print(f'Ham Matrix: \\n {ham_fm}\\n')\n",
    "\n",
    "spam_fm = create_feature_matrix(train_spam_df['word_list'])\n",
    "print(f\"Spam Matrix: \\n {spam_fm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Computing the priors}$\n",
    "\n",
    "$$P(c = \\text{ham}) = \\frac{N_{\\text{ham}}}{N_{\\text{doc}}}$$\n",
    "$$P(c = \\text{spam}) = \\frac{N_{\\text{spam}}}{N_{\\text{doc}}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_ham: 0.3531924882629108 | P_spam: 0.6468075117370892\n"
     ]
    }
   ],
   "source": [
    "n_ham = train_ham_df.shape[0]       # number of ham emails in training set\n",
    "n_spam = train_spam_df.shape[0]     # number of spam emails in training set\n",
    "n_doc = train_df.shape[0]           # number of total emails in training set\n",
    "\n",
    "p_ham = n_ham / n_doc\n",
    "p_spam = n_spam / n_doc\n",
    "\n",
    "print(f\"P_ham: {p_ham} | P_spam: {p_spam}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Computing the likelihood of each word}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(w_i | \\text{spam}) = \\frac{\\text{count}(w_i, \\text{spam}) + \\lambda}{\\left( \\sum_{w \\in V} \\text{count}(w, \\text{spam}) \\right) + \\lambda |V|}$$\n",
    "\n",
    "$$P(w_i | \\text{ham}) = \\frac{\\text{count}(w_i, \\text{ham}) + \\lambda}{\\left( \\sum_{w \\in V} \\text{count}(w, \\text{ham}) \\right) + \\lambda |V|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized sum of word counts in ham and spam\n",
    "ham_word_count = np.sum(ham_fm, axis=0)\n",
    "spam_word_count = np.sum(spam_fm, axis=0)\n",
    "\n",
    "# Calculate total word counts for ham and spam\n",
    "ham_word_total = np.sum(np.sum(ham_fm, axis=0))\n",
    "spam_word_total = np.sum(np.sum(spam_fm, axis=0))\n",
    "\n",
    "# Initialize dictionaries for probabilities of each word in ham and spam classes\n",
    "p_ham_count = {}\n",
    "p_spam_count = {}\n",
    "\n",
    "# Laplace smoothing parameter\n",
    "lmbda = 1\n",
    "\n",
    "for i in range(COMMON_WORD_COUNT):\n",
    "    curr_ham_word = (ham_word_count[i] + lmbda) / (\n",
    "        ham_word_total + lmbda * COMMON_WORD_COUNT\n",
    "    )\n",
    "    curr_spam_word = (spam_word_count[i] + lmbda) / (\n",
    "        spam_word_total + lmbda * COMMON_WORD_COUNT\n",
    "    )\n",
    "    p_ham_count[common_words[i]] = curr_ham_word\n",
    "    p_spam_count[common_words[i]] = curr_spam_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Classifying the emails}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to classify the emails\n",
    "# used log function to calculate whether email is spam or ham\n",
    "def classify_emails(\n",
    "    tokenized_email, p_ham, p_spam, p_count_ham, p_count_spam, word_list\n",
    "):\n",
    "    # initialize the log values of ham and spam with the log of their probabilities\n",
    "    log_p_ham = np.log(p_ham)\n",
    "    log_p_spam = np.log(p_spam)\n",
    "\n",
    "    for w in tokenized_email:\n",
    "        # add the probability value if the word is at the word list aka the top 10k most common words\n",
    "        if w in word_list:\n",
    "            log_p_ham += np.log(p_count_ham[w])\n",
    "            log_p_spam += np.log(p_count_spam[w])\n",
    "\n",
    "    # return 0 if the value of ham is greater than spam\n",
    "    return \"ham\" if log_p_ham > log_p_spam else \"spam\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Testing the Classifier}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test = []\n",
    "\n",
    "# loop through the email content of the test set and file path\n",
    "for msg in test_df[\"word_list\"]:\n",
    "    classify = classify_emails(\n",
    "        msg, p_ham, p_spam, p_ham_count, p_spam_count, common_words\n",
    "    )  # classify the current content whether it's spam or ham\n",
    "    predicted_test.append(classify)  # append either 1 or 0 as the predicted label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\glena\\AppData\\Local\\Temp\\ipykernel_19032\\535618268.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[\"prediction\"] = predicted_test\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>category</th>\n",
       "      <th>word_list</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21300</th>\n",
       "      <td>data/071/000</td>\n",
       "      <td>spam</td>\n",
       "      <td>[hesitantly, derive, perverse, satisfaction, c...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21301</th>\n",
       "      <td>data/071/001</td>\n",
       "      <td>ham</td>\n",
       "      <td>[things, perform, experiment, display, will, r...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21302</th>\n",
       "      <td>data/071/002</td>\n",
       "      <td>spam</td>\n",
       "      <td>[best, offer, month, viggra, ci, ialis, vaiium...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21303</th>\n",
       "      <td>data/071/003</td>\n",
       "      <td>spam</td>\n",
       "      <td>[de, ar, wne, cr, doesn, matter, ow, real, st,...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21304</th>\n",
       "      <td>data/071/004</td>\n",
       "      <td>spam</td>\n",
       "      <td>[special, offer, adobe, video, collection, ado...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37817</th>\n",
       "      <td>data/126/017</td>\n",
       "      <td>spam</td>\n",
       "      <td>[great, news, expec, ted, infinex, ventures, i...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37818</th>\n",
       "      <td>data/126/018</td>\n",
       "      <td>spam</td>\n",
       "      <td>[oil, sector, going, crazy, weekly, gift, kkpt...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37819</th>\n",
       "      <td>data/126/019</td>\n",
       "      <td>spam</td>\n",
       "      <td>[http, vdtobj, docscan, info, suffering, pain,...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37820</th>\n",
       "      <td>data/126/020</td>\n",
       "      <td>spam</td>\n",
       "      <td>[prosperous, future, increased, money, earning...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37821</th>\n",
       "      <td>data/126/021</td>\n",
       "      <td>spam</td>\n",
       "      <td>[moat, coverall, cytochemistry, planeload, salk]</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16522 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          file_path category  \\\n",
       "21300  data/071/000     spam   \n",
       "21301  data/071/001      ham   \n",
       "21302  data/071/002     spam   \n",
       "21303  data/071/003     spam   \n",
       "21304  data/071/004     spam   \n",
       "...             ...      ...   \n",
       "37817  data/126/017     spam   \n",
       "37818  data/126/018     spam   \n",
       "37819  data/126/019     spam   \n",
       "37820  data/126/020     spam   \n",
       "37821  data/126/021     spam   \n",
       "\n",
       "                                               word_list prediction  \n",
       "21300  [hesitantly, derive, perverse, satisfaction, c...       spam  \n",
       "21301  [things, perform, experiment, display, will, r...        ham  \n",
       "21302  [best, offer, month, viggra, ci, ialis, vaiium...       spam  \n",
       "21303  [de, ar, wne, cr, doesn, matter, ow, real, st,...       spam  \n",
       "21304  [special, offer, adobe, video, collection, ado...       spam  \n",
       "...                                                  ...        ...  \n",
       "37817  [great, news, expec, ted, infinex, ventures, i...       spam  \n",
       "37818  [oil, sector, going, crazy, weekly, gift, kkpt...       spam  \n",
       "37819  [http, vdtobj, docscan, info, suffering, pain,...       spam  \n",
       "37820  [prosperous, future, increased, money, earning...       spam  \n",
       "37821   [moat, coverall, cytochemistry, planeload, salk]       spam  \n",
       "\n",
       "[16522 rows x 4 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"prediction\"] = predicted_test\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Performance Evaluation}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9196223217528144\n",
      "Recall: 0.9163897620116749\n",
      "Precision: 0.9625507027638902\n"
     ]
    }
   ],
   "source": [
    "y_true = test_df[\"category\"]\n",
    "y_pred = test_df[\"prediction\"]\n",
    "\n",
    "# Manual calculations based on the logic provided earlier\n",
    "tp = ((y_true == \"spam\") & (y_pred == \"spam\")).sum()\n",
    "tn = ((y_true == \"ham\") & (y_pred == \"ham\")).sum()\n",
    "fp = ((y_true == \"ham\") & (y_pred == \"spam\")).sum()\n",
    "fn = ((y_true == \"spam\") & (y_pred == \"ham\")).sum()\n",
    "\n",
    "# Calculate metrics manually\n",
    "accuracy = (tn + tp) / (tn + tp + fp + fn)\n",
    "recall = tp / (tp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "\n",
    "print(f'Accuracy: {accuracy}\\nRecall: {recall}\\nPrecision: {precision}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
