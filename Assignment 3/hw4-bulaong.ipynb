{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Github Link: https://github.com/dreeew05/CMSC-197/tree/main/Assignment%203"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Regex for removing unwanted characters\n",
    "import re\n",
    "\n",
    "# To read email\n",
    "from email import policy\n",
    "from email.parser import BytesParser\n",
    "\n",
    "# To count common words\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_PATH = \"trec06p-cs280/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the stop words and convert into array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'able', 'about', 'above', 'abst']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = open('stop_words.txt').read().splitlines()\n",
    "\n",
    "# For visualization purposes\n",
    "stop_words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'file_path': [],\n",
    "    'category': []\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Preprocessing}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_path = f\"{FOLDER_PATH}labels\"\n",
    "with open(labels_path) as f:\n",
    "    # Remove ../to mitigate file access errors\n",
    "    str_to_remove = \"../\"\n",
    "    for line in f:\n",
    "        category, path = line.split()\n",
    "        clean_path = path.replace(str_to_remove, '')\n",
    "        new_row = pd.DataFrame([[clean_path, category]], columns=[\"file_path\", \"category\"])\n",
    "        df = pd.concat([df, new_row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the email:\n",
    "\n",
    "- Remove alphanumeric characters\n",
    "- Remove punctuation marks\n",
    "- Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_email(email_body):\n",
    "    # [^a-zA-Z\\s]+ => For non-alphabetic and non-whitespace (punctuations, new line, tab)\n",
    "    # \\s+ = For one or more whitespace characters\n",
    "    pattern = r\"[^a-zA-Z\\s]+|\\s+\"  # Combine both patterns\n",
    "    clean_message = re.sub(pattern, \" \", email_body).strip().lower()\n",
    "\n",
    "    # Split and remove stop words in a single step\n",
    "    return [word for word in clean_message.split() if word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating to each mail:\n",
    "- Clean each mail\n",
    "- Tokenize the clean mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents_arr = []\n",
    "\n",
    "for path in df[\"file_path\"]:\n",
    "    current_file_path = f\"{FOLDER_PATH}{path}\"\n",
    "    with open(current_file_path, \"rb\") as f:\n",
    "        raw_email = f.read()\n",
    "\n",
    "    # Parse email content\n",
    "    msg = BytesParser(policy=policy.default).parsebytes(raw_email)\n",
    "\n",
    "    # Extract body (defaulting to empty string in case of issues)\n",
    "    body = \"\"\n",
    "\n",
    "    # Define a function to decode email parts safely\n",
    "    def decode_payload(part):\n",
    "        try:\n",
    "            charset = part.get_content_charset() or \"utf-8\"\n",
    "            return part.get_payload(decode=True).decode(charset)\n",
    "        except (LookupError, UnicodeDecodeError):\n",
    "            return part.get_payload(decode=True).decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "    # Check for multipart or single-part message\n",
    "    if msg.is_multipart():\n",
    "        for part in msg.iter_parts():\n",
    "            if part.get_content_type() == \"text/plain\":\n",
    "                body = decode_payload(part)\n",
    "                break\n",
    "    else:\n",
    "        body = decode_payload(msg)\n",
    "\n",
    "    # Clean the email body and append the word list\n",
    "    word_list = clean_email(body)\n",
    "    contents_arr.append(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding another column to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>category</th>\n",
       "      <th>word_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/000/000</td>\n",
       "      <td>ham</td>\n",
       "      <td>[mailing, list, queried, weeks, ago, running, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/000/001</td>\n",
       "      <td>spam</td>\n",
       "      <td>[luxury, watches, buy, rolex, rolex, cartier, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/000/002</td>\n",
       "      <td>spam</td>\n",
       "      <td>[academic, qualifications, prestigious, acc, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/000/003</td>\n",
       "      <td>ham</td>\n",
       "      <td>[greetings, verify, subscription, plan, fans, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/000/004</td>\n",
       "      <td>spam</td>\n",
       "      <td>[chauncey, conferred, luscious, continued, ton...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37817</th>\n",
       "      <td>data/126/017</td>\n",
       "      <td>spam</td>\n",
       "      <td>[great, news, expec, ted, infinex, ventures, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37818</th>\n",
       "      <td>data/126/018</td>\n",
       "      <td>spam</td>\n",
       "      <td>[oil, sector, going, crazy, weekly, gift, kkpt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37819</th>\n",
       "      <td>data/126/019</td>\n",
       "      <td>spam</td>\n",
       "      <td>[http, vdtobj, docscan, info, suffering, pain,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37820</th>\n",
       "      <td>data/126/020</td>\n",
       "      <td>spam</td>\n",
       "      <td>[prosperous, future, increased, money, earning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37821</th>\n",
       "      <td>data/126/021</td>\n",
       "      <td>spam</td>\n",
       "      <td>[moat, coverall, cytochemistry, planeload, salk]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37822 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          file_path category  \\\n",
       "0      data/000/000      ham   \n",
       "1      data/000/001     spam   \n",
       "2      data/000/002     spam   \n",
       "3      data/000/003      ham   \n",
       "4      data/000/004     spam   \n",
       "...             ...      ...   \n",
       "37817  data/126/017     spam   \n",
       "37818  data/126/018     spam   \n",
       "37819  data/126/019     spam   \n",
       "37820  data/126/020     spam   \n",
       "37821  data/126/021     spam   \n",
       "\n",
       "                                               word_list  \n",
       "0      [mailing, list, queried, weeks, ago, running, ...  \n",
       "1      [luxury, watches, buy, rolex, rolex, cartier, ...  \n",
       "2      [academic, qualifications, prestigious, acc, r...  \n",
       "3      [greetings, verify, subscription, plan, fans, ...  \n",
       "4      [chauncey, conferred, luscious, continued, ton...  \n",
       "...                                                  ...  \n",
       "37817  [great, news, expec, ted, infinex, ventures, i...  \n",
       "37818  [oil, sector, going, crazy, weekly, gift, kkpt...  \n",
       "37819  [http, vdtobj, docscan, info, suffering, pain,...  \n",
       "37820  [prosperous, future, increased, money, earning...  \n",
       "37821   [moat, coverall, cytochemistry, planeload, salk]  \n",
       "\n",
       "[37822 rows x 3 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['word_list'] = contents_arr\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into three groups:\n",
    "\n",
    "- Training set for ham\n",
    "- Training set for spam\n",
    "- Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df['file_path'] < 'data/071']\n",
    "train_ham_df = train_df[train_df[\"category\"] == \"ham\"]\n",
    "train_spam_df = train_df[train_df['category'] == 'spam']\n",
    "\n",
    "test_df = df[df['file_path'] >= 'data/071']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the 10000 most common words from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('http', 27587),\n",
       " ('font', 27472),\n",
       " ('td', 27416),\n",
       " ('br', 24631),\n",
       " ('width', 13978)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_words = [word for sublist in train_df['word_list'] for word in sublist]\n",
    "\n",
    "word_count = Counter(training_words)\n",
    "top_common_words = word_count.most_common(1000)\n",
    "\n",
    "# For vizualization purposes\n",
    "top_common_words[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
